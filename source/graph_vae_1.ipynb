{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import itertools\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import dense_to_sparse, remove_self_loops\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "drop_hydrogen = True\n",
    "\n",
    "max_num_nodes = 9 if drop_hydrogen else 9\n",
    "\n",
    "# TODO: pre-transform and store matrices to disk\n",
    "transform_list = [\n",
    "    SelectQM9TargetProperties(properties=[\"homo\", \"lumo\"]),\n",
    "    SelectQM9NodeFeatures(features=[\"atom_type\"]),\n",
    "]\n",
    "if drop_hydrogen:\n",
    "    transform_list.append(DropQM9Hydrogen())\n",
    "\n",
    "transform_list += [\n",
    "    AddAdjacencyMatrix(max_num_nodes=max_num_nodes),\n",
    "    AddNodeAttributeMatrix(max_num_nodes=max_num_nodes),\n",
    "    AddEdgeAttributeMatrix(max_num_nodes=max_num_nodes),\n",
    "    T.ToDevice(device=device)\n",
    "]\n",
    "transform = T.Compose(transform_list)\n",
    "\n",
    "dataset = QM9(root=\"./data\", transform=transform)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_qm9_data_split(dataset=dataset)\n",
    "\n",
    "num_node_features = dataset.num_node_features\n",
    "num_edge_features = dataset.num_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = hparams[\"latent_dim\"]\n",
    "\n",
    "        # TODO: two graph convolutional layers (32 and 64 channels) with identity connection (edge conditioned graph convolution)\n",
    "        self.conv1 = GCNConv(in_channels=hparams[\"num_node_features\"], out_channels=32)\n",
    "        self.conv2 = GCNConv(in_channels=32, out_channels=64)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=64, out_features=128)\n",
    "        # output 2 time the size of the latent vector\n",
    "        # one half contains mu and the other half log(sigma)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=self.latent_dim * 2)\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        mu = x[:, :self.latent_dim]\n",
    "        log_sigma = x[:, self.latent_dim:]\n",
    "        return mu, log_sigma\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fcls = nn.Sequential(\n",
    "            nn.Linear(in_features=hparams[\"latent_dim\"], out_features=128),\n",
    "            #nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            #nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            #nn.BatchNorm1d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.max_num_nodes = hparams[\"max_num_nodes\"]\n",
    "        self.num_node_features = hparams[\"num_node_features\"]\n",
    "\n",
    "        # the atom graph is symmetric so we only predict the upper triangular part\n",
    "        # and the diagonal that indicates the presence of nodes\n",
    "        upper_triangular_diag_size = int(self.max_num_nodes * (self.max_num_nodes + 1) / 2)\n",
    "        self.fc_adjacency = nn.Linear(in_features=512, out_features=upper_triangular_diag_size)\n",
    "\n",
    "        self.fc_node_features = nn.Linear(in_features=512, out_features=self.max_num_nodes * num_node_features)\n",
    "\n",
    "        self.max_num_edges = int(self.max_num_nodes * (self.max_num_nodes - 1) / 2)\n",
    "        self.num_edge_features = hparams[\"num_edge_features\"]\n",
    "        self.fc_edge_features = nn.Linear(in_features=512, out_features=self.max_num_edges * self.num_edge_features)\n",
    "        \n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.fcls(z)\n",
    "        # predict upper triangular matrix including the diagonal\n",
    "        adj_triu_mat = self.fc_adjacency(x)\n",
    "        node_features = self.fc_node_features(x)\n",
    "        edge_features = self.fc_edge_features(x)\n",
    "\n",
    "        # reshape matrices\n",
    "        node_mat = node_features.view(-1, self.max_num_nodes, self.num_node_features)\n",
    "        edge_triu_mat = edge_features.view(-1, self.max_num_edges, self.num_edge_features)\n",
    "\n",
    "        return adj_triu_mat, node_mat, edge_triu_mat\n",
    "\n",
    "\n",
    "class GraphVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(hparams=hparams)\n",
    "        self.decoder = Decoder(hparams=hparams)\n",
    "        self.latent_dim = hparams[\"latent_dim\"]\n",
    "\n",
    "        rows, cols = torch.triu_indices(hparams[\"max_num_nodes\"], hparams[\"max_num_nodes\"])\n",
    "        self.diag_triu_mask = rows == cols\n",
    "\n",
    "        self.edge_triu_rows, self.edge_triu_cols = torch.triu_indices(hparams[\"max_num_nodes\"], hparams[\"max_num_nodes\"], offset=1)\n",
    "\n",
    "    def _sample_with_reparameterization(self, mu: torch.Tensor, log_sigma: torch.Tensor) -> torch.Tensor:\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_norm = torch.randn_like(mu)\n",
    "        return std_norm * sigma + mu\n",
    "\n",
    "    def forward(self, data: Data) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, log_sigma = self.encoder(data)\n",
    "        z = self._sample_with_reparameterization(mu=mu, log_sigma=log_sigma)\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "    \n",
    "    def _kl_divergence(self, mu: torch.Tensor, log_sigma: torch.Tensor) -> torch.Tensor:\n",
    "        log_sigma_squared = log_sigma + log_sigma\n",
    "        sigma_squared = torch.exp(log_sigma_squared)\n",
    "        mu_squared = mu * mu\n",
    "        kl_div_sample = 0.5 * torch.sum(sigma_squared + mu_squared - log_sigma_squared - 1, dim=1)\n",
    "        # average over the batch\n",
    "        return torch.mean(kl_div_sample)\n",
    "    \n",
    "    def _reconstruction_loss(\n",
    "        self, \n",
    "        input: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "        target: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "    ):\n",
    "        input_adj_triu_mat, input_node_mat, input_edge_mat = input\n",
    "        target_adj_triu_mat, target_node_mat, target_edge_mat = target\n",
    "\n",
    "        # average loss over nodes and edges separately\n",
    "        input_adj_triu_mat_diag = input_adj_triu_mat[:, self.diag_triu_mask]\n",
    "        input_adj_triu_mat_off_diag = input_adj_triu_mat[:, ~self.diag_triu_mask]\n",
    "        target_adj_triu_mat_diag = target_adj_triu_mat[:, self.diag_triu_mask]\n",
    "        target_adj_triu_mat_off_diag = target_adj_triu_mat[:, ~self.diag_triu_mask]\n",
    "        adjacency_loss = (\n",
    "            F.binary_cross_entropy_with_logits(input=input_adj_triu_mat_diag, target=target_adj_triu_mat_diag)\n",
    "            + F.binary_cross_entropy_with_logits(input=input_adj_triu_mat_off_diag, target=target_adj_triu_mat_off_diag)\n",
    "        )\n",
    "\n",
    "        # compute the node feature loss only for nodes that exist in the input graph\n",
    "        node_mat_logits = input_node_mat.view(-1, input_node_mat.size(-1))\n",
    "        node_targets = target_node_mat.argmax(dim=2).view(-1)\n",
    "        per_node_feature_loss = F.cross_entropy(input=node_mat_logits, target=node_targets, reduction=\"none\")\n",
    "        node_mask = target_adj_triu_mat_diag.view(-1)\n",
    "        node_feature_loss = (per_node_feature_loss * node_mask).sum() / node_mask.sum()\n",
    "\n",
    "        # Compute edge feature loss only for edges that are connected to nodes existing in the input graph\n",
    "        edge_mat_logits = input_edge_mat.view(-1, input_edge_mat.size(-1))\n",
    "        edge_targets = target_edge_mat.argmax(dim=2).view(-1)\n",
    "        per_edge_feature_loss = F.cross_entropy(input=edge_mat_logits, target=edge_targets, reduction=\"none\")\n",
    "        edge_mask = (\n",
    "            target_adj_triu_mat_diag[:, self.edge_triu_rows].int() & target_adj_triu_mat_diag[:, self.edge_triu_cols].int()\n",
    "        ).view(-1)\n",
    "        edge_feature_loss = (per_edge_feature_loss * edge_mask).sum() / edge_mask.sum()\n",
    "        \n",
    "        return adjacency_loss + node_feature_loss + edge_feature_loss\n",
    "    \n",
    "    def negative_elbo(self, x: Data):\n",
    "        mu, log_sigma = self.encoder(x)\n",
    "        z = self._sample_with_reparameterization(mu=mu, log_sigma=log_sigma)\n",
    "\n",
    "        x_recon = self.decoder(z)\n",
    "        x_target = (x.adj_triu_mat, x.node_mat, x.edge_triu_mat)\n",
    "        return self._reconstruction_loss(input=x_recon, target=x_target) + self._kl_divergence(mu=mu, log_sigma=log_sigma)\n",
    "    \n",
    "    def sample(self, num_samples: int, device: str):\n",
    "        z = torch.randn((num_samples, self.latent_dim), device=device)\n",
    "        x = self.decoder(z)\n",
    "        return z, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"batch_size\": 256,\n",
    "    \"max_num_nodes\": max_num_nodes,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"beta_1\": 0.5,\n",
    "    \"epochs\": 500,\n",
    "    \"num_node_features\": num_node_features,\n",
    "    \"num_edge_features\": num_edge_features,\n",
    "    \"latent_dim\": 128  # c in the paper\n",
    "}\n",
    "\n",
    "batch_size = hparams[\"batch_size\"]\n",
    "dataloaders = {\n",
    "    \"train_single\": DataLoader(train_dataset[16:24], batch_size=batch_size, shuffle=True),\n",
    "    \"train_tiny\": DataLoader(train_dataset[:batch_size], batch_size=batch_size, shuffle=True),\n",
    "    \"train_small\": DataLoader(train_dataset[:4096], batch_size=batch_size, shuffle=True),\n",
    "    \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "    \"val\": DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "}\n",
    "\n",
    "val_subset_count = 32\n",
    "dataloaders[\"val_subsets\"] = create_validation_subset_loaders(validation_dataset=val_dataset, subset_count=32, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
      "Epoch 2 Training: 100%|██████████| 16/16 [00:03<00:00,  4.02it/s]\n",
      "Epoch 3 Training: 100%|██████████| 16/16 [00:04<00:00,  3.95it/s]\n",
      "Epoch 4 Training: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "Epoch 5 Training: 100%|██████████| 16/16 [00:04<00:00,  3.95it/s]\n",
      "Epoch 6 Training: 100%|██████████| 16/16 [00:04<00:00,  3.97it/s]\n",
      "Epoch 7 Training: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
      "Epoch 8 Training: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]\n",
      "Epoch 9 Training: 100%|██████████| 16/16 [00:04<00:00,  3.96it/s]\n",
      "Epoch 10 Training: 100%|██████████| 16/16 [00:04<00:00,  3.60it/s]\n",
      "Epoch 11 Training: 100%|██████████| 16/16 [00:03<00:00,  4.01it/s]\n",
      "Epoch 12 Training: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]\n",
      "Epoch 13 Training: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
      "Epoch 14 Training: 100%|██████████| 16/16 [00:04<00:00,  3.98it/s]\n",
      "Epoch 15 Training: 100%|██████████| 16/16 [00:04<00:00,  3.91it/s]\n",
      "Epoch 16 Training: 100%|██████████| 16/16 [00:04<00:00,  3.66it/s]\n",
      "Epoch 17 Training: 100%|██████████| 16/16 [00:05<00:00,  3.14it/s]\n",
      "Epoch 18 Training: 100%|██████████| 16/16 [00:04<00:00,  3.96it/s]\n",
      "Epoch 19 Training: 100%|██████████| 16/16 [00:04<00:00,  3.65it/s]\n",
      "Epoch 20 Training: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]\n",
      "Epoch 21 Training: 100%|██████████| 16/16 [00:04<00:00,  3.89it/s]\n",
      "Epoch 22 Training: 100%|██████████| 16/16 [00:04<00:00,  3.56it/s]\n",
      "Epoch 23 Training: 100%|██████████| 16/16 [00:03<00:00,  4.01it/s]\n",
      "Epoch 24 Training: 100%|██████████| 16/16 [00:04<00:00,  4.00it/s]\n",
      "Epoch 25 Training: 100%|██████████| 16/16 [00:04<00:00,  3.91it/s]\n",
      "Epoch 26 Training: 100%|██████████| 16/16 [00:04<00:00,  3.63it/s]\n",
      "Epoch 27 Training: 100%|██████████| 16/16 [00:04<00:00,  3.88it/s]\n",
      "Epoch 28 Training: 100%|██████████| 16/16 [00:04<00:00,  3.98it/s]\n",
      "Epoch 29 Training: 100%|██████████| 16/16 [00:04<00:00,  3.59it/s]\n",
      "Epoch 30 Training: 100%|██████████| 16/16 [00:04<00:00,  3.97it/s]\n",
      "Epoch 31 Training: 100%|██████████| 16/16 [00:04<00:00,  3.97it/s]\n",
      "Epoch 32 Training: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
      "Epoch 33 Training: 100%|██████████| 16/16 [00:03<00:00,  4.06it/s]\n",
      "Epoch 34 Training: 100%|██████████| 16/16 [00:04<00:00,  3.99it/s]\n",
      "Epoch 35 Training: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
      "Epoch 36 Training: 100%|██████████| 16/16 [00:04<00:00,  3.97it/s]\n",
      "Epoch 37 Training: 100%|██████████| 16/16 [00:03<00:00,  4.04it/s]\n",
      "Epoch 38 Training: 100%|██████████| 16/16 [00:04<00:00,  3.61it/s]\n",
      "Epoch 39 Training: 100%|██████████| 16/16 [00:04<00:00,  3.95it/s]\n",
      "Epoch 40 Training: 100%|██████████| 16/16 [00:04<00:00,  3.99it/s]\n",
      "Epoch 41 Training: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n",
      "Epoch 42 Training: 100%|██████████| 16/16 [00:03<00:00,  4.01it/s]\n",
      "Epoch 43 Training: 100%|██████████| 16/16 [00:03<00:00,  4.01it/s]\n",
      "Epoch 44 Training:  50%|█████     | 8/16 [00:02<00:02,  3.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[336], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     17\u001b[0m     graph_vae_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m Training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_prediction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgraph_vae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch_geometric/data/dataset.py:264\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[1;32m    263\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()[idx])\n\u001b[0;32m--> 264\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch_geometric/transforms/base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch_geometric/transforms/compose.py:24\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m         data \u001b[38;5;241m=\u001b[39m [transform(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch_geometric/transforms/base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/idp_generative_modeling/source/data_utils.py:104\u001b[0m, in \u001b[0;36mAddAdjacencyMatrix.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    101\u001b[0m     data: Union[Data, HeteroData],\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Data, HeteroData]:\n\u001b[1;32m    103\u001b[0m     edge_index_with_loops, _ \u001b[38;5;241m=\u001b[39m add_self_loops(edge_index\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m--> 104\u001b[0m     adj_mat \u001b[38;5;241m=\u001b[39m \u001b[43mto_dense_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index_with_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_num_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     data\u001b[38;5;241m.\u001b[39madj_triu_mat \u001b[38;5;241m=\u001b[39m adj_mat[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriu_mask]\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/idp/lib/python3.11/site-packages/torch_geometric/utils/to_dense_adj.py:10\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptTensor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cumsum, scatter\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense_adj\u001b[39m(\n\u001b[1;32m     11\u001b[0m     edge_index: Tensor,\n\u001b[1;32m     12\u001b[0m     batch: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     edge_attr: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     max_num_nodes: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     batch_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts batched sparse adjacency matrices given by edge indices and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    edge attributes to a single dense batched adjacency matrix.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m                [5., 0.]]])\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_vae_model = GraphVAE(hparams=hparams).to(device=device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    graph_vae_model.parameters(),\n",
    "    lr=hparams[\"learning_rate\"],\n",
    "    betas=(hparams[\"beta_1\"], 0.999)\n",
    ")\n",
    "epochs = hparams[\"epochs\"]\n",
    "\n",
    "train_loader = dataloaders[\"train_small\"]\n",
    "val_subset_loader_iterator = itertools.cycle(dataloaders[\"val_subsets\"])\n",
    "\n",
    "validation_interval = 50\n",
    "\n",
    "writer = create_tensorboard_writer(experiment_name=\"graph-vae\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    graph_vae_model.train()\n",
    "    for batch_index, train_batch in enumerate(tqdm(train_loader,  desc=f\"Epoch {epoch + 1} Training\")):\n",
    "        optimizer.zero_grad()\n",
    "        train_prediction = graph_vae_model(train_batch)\n",
    "        train_target = (train_batch.adj_triu_mat, train_batch.node_mat, train_batch.edge_triu_mat)\n",
    "\n",
    "        loss = graph_vae_model.negative_elbo(x=train_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration = len(train_loader) * epoch + batch_index\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": loss.item()}, iteration)\n",
    "\n",
    "        if iteration % validation_interval == 0:\n",
    "            graph_vae_model.eval()\n",
    "            val_loss_sum = 0\n",
    "\n",
    "            # Get the next subset of the validation set\n",
    "            val_loader = next(val_subset_loader_iterator)\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_loss_sum += graph_vae_model.negative_elbo(x=val_batch)\n",
    "            \n",
    "            val_loss = val_loss_sum / len(val_loader)\n",
    "            writer.add_scalars(\"Loss\", {\"Validation\": val_loss.item()}, iteration)\n",
    "            \n",
    "            graph_vae_model.train()\n",
    "\n",
    "    # Visualization\n",
    "    # get random mol from train dataset and visualize\n",
    "    sample_index = random.randint(0, len(train_loader) * batch_size)\n",
    "    sample = train_dataset[sample_index]\n",
    "    writer.add_image('Input', molecule_graph_data_to_image(sample, includes_h=not drop_hydrogen), global_step=epoch, dataformats=\"NCHW\")\n",
    "\n",
    "    # encode and decode the molecule\n",
    "    graph_vae_model.eval()\n",
    "    z, x = graph_vae_model.sample(num_samples=1, device=device)\n",
    "    pred_adj_triu_mat, pred_node_mat, pred_edge_triu_mat = graph_vae_model(sample)\n",
    "\n",
    "    #####################################\n",
    "    # Convert predicted matrices to graph\n",
    "    #####################################\n",
    "\n",
    "    # drop the batch dimension\n",
    "    pred_adj_triu_mat = pred_adj_triu_mat[0]\n",
    "    pred_node_mat = pred_node_mat[0]\n",
    "    pred_edge_triu_mat = pred_edge_triu_mat[0]\n",
    "\n",
    "    n = hparams[\"max_num_nodes\"]\n",
    "\n",
    "    edge_triu_mat = pred_edge_triu_mat.argmax(dim=1).float()\n",
    "    edge_mat = torch.zeros(n, n, device=device)\n",
    "    edge_mat[torch.ones(n, n).triu(diagonal=1) == 1] = edge_triu_mat\n",
    "    edge_mat = edge_mat + 1  # add one so we can so that 0 indicates no node instead of hydrogen\n",
    "\n",
    "    # convert predicted upper triagular matrix into symmetric edge index and\n",
    "    adj_triu_mat = torch.where(F.sigmoid(pred_adj_triu_mat) > 0.5, 1.0, 0.0)\n",
    "    adj_mat = torch.zeros(n, n, device=device)\n",
    "    adj_mat[torch.ones(n, n).triu() == 1] = adj_triu_mat\n",
    "    diagonal = adj_mat.diagonal()\n",
    "\n",
    "    # combine the adjacency matrix with edge features\n",
    "    edge_mat *= adj_mat\n",
    "\n",
    "    node_mask = diagonal == 1\n",
    "    edge_index, edge_attr = dense_to_sparse(adj=edge_mat.unsqueeze(0), mask=node_mask.unsqueeze(0))\n",
    "\n",
    "    edge_attr = F.one_hot((edge_attr - 1).long(), num_classes=num_edge_features)\n",
    "\n",
    "    # Adjust indices in edge_index to account for removed nodes\n",
    "    # Create a mapping from old indices to new indices\n",
    "    old_to_new_indices = torch.cumsum(node_mask, 0) - 1\n",
    "    old_to_new_indices[~node_mask] = -1\n",
    "    new_edge_index = old_to_new_indices[edge_index]\n",
    "\n",
    "    # Remove edges that contain removed nodes\n",
    "    valid_edges = (new_edge_index >= 0).all(dim=0)\n",
    "    new_edge_index = new_edge_index[:, valid_edges]\n",
    "    new_edge_attr = edge_attr[valid_edges, :]\n",
    "\n",
    "    edge_index, edge_attr = remove_self_loops(edge_index=new_edge_index, edge_attr=new_edge_attr)\n",
    "\n",
    "    # convert node feature logits into one-hot vector\n",
    "    x = F.one_hot(pred_node_mat[node_mask].argmax(dim=1), num_classes=num_node_features)\n",
    "    reconstructed_sample = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    \n",
    "    writer.add_image('Reconstruction', molecule_graph_data_to_image(reconstructed_sample, includes_h=not drop_hydrogen), global_step=epoch, dataformats=\"NCHW\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
