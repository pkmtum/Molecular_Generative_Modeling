{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    SelectQM9TargetProperties(properties=[\"homo\", \"lumo\"]),\n",
    "    SelectQM9NodeFeatures(features=[\"atom_type\"]),\n",
    "    AddAdjacencyMatrix(max_num_nodes=29),\n",
    "    T.ToDevice(device=device)\n",
    "])\n",
    "\n",
    "dataset = QM9(root=\"./data\", transform=transform)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_qm9_data_split(dataset=dataset)\n",
    "\n",
    "num_node_features = dataset.num_node_features\n",
    "num_edge_features = dataset.num_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: two graph convolutional layers (32 and 64 channeles) with identity connection (edge conditioned graph convolution)\n",
    "        self.conv1 = GCNConv(in_channels=hparams[\"num_node_features\"], out_channels=32)\n",
    "        self.conv2 = GCNConv(in_channels=32, out_channels=64)\n",
    "        self.fc = nn.Linear(in_features=64, out_features=128)\n",
    "        # TODO: use nn.Sequential\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fcls = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        n = hparams[\"max_node_count\"]\n",
    "        # the atom graph is symmetric so we only predict the upper triangular part\n",
    "        upper_triangular_size = int(n * (n + 1) / 2)\n",
    "        self.fc_adjacency = nn.Linear(in_features=512, out_features=upper_triangular_size)\n",
    "\n",
    "        self.fc_node_features = nn.Linear(in_features=512, out_features=n * hparams[\"num_node_features\"])\n",
    "        self.fc_edge_features = nn.Linear(in_features=512, out_features=n * hparams[\"num_edge_features\"])\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.fcls(z)\n",
    "        adjacency_matrix = self.fc_adjacency(x)\n",
    "        node_features = self.fc_node_features(x)\n",
    "        edge_features = self.fc_edge_features(x)\n",
    "\n",
    "        # upper triangular\n",
    "        return adjacency_matrix, node_features, edge_features\n",
    "\n",
    "\n",
    "class GraphVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(hparams=hparams)\n",
    "        self.decoder = Decoder(hparams=hparams)\n",
    "\n",
    "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z = self.encoder(data)\n",
    "        x = self.decoder(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'max_node_count': 29, 'learning_rate': 0.001, 'epochs': 25, 'num_node_features': 5, 'num_edge_features': 4}\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    \"batch_size\": 32,\n",
    "    \"max_node_count\": 29,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"epochs\": 25,\n",
    "    \"num_node_features\": num_node_features,\n",
    "    \"num_edge_features\": num_edge_features\n",
    "}\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = hparams[\"batch_size\"]\n",
    "\n",
    "# TODO: implement reconstruction loss adjacency\n",
    "# TODO: implement reconstruction loss node features\n",
    "# TODO: implement reconstruction loss edge features\n",
    "# TODO: add kl-loss\n",
    "# TODO: implement encoder from the paper\n",
    "# TODO: graph matching\n",
    "\n",
    "dataloaders = {\n",
    "    \"train_single\": DataLoader(train_dataset[:1], batch_size=batch_size, shuffle=True),\n",
    "    \"train_tiny\": DataLoader(train_dataset[:16], batch_size=batch_size, shuffle=True),\n",
    "    \"train_small\": DataLoader(train_dataset[:4096], batch_size=batch_size, shuffle=True),\n",
    "    \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "\n",
    "    \"val_small\": DataLoader(val_dataset[:512], batch_size=batch_size, shuffle=False),\n",
    "    \"val\": DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "}\n",
    "\n",
    "val_subset_count = 32\n",
    "dataloaders[\"val_subsets\"] = create_validation_subset_loaders(validation_dataset=val_dataset, subset_count=32, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n",
      "tensor([ 3.2702e-02, -3.6735e-02,  5.0752e-02, -4.9446e-01,  2.0766e-02,\n",
      "         1.9365e-01,  1.3127e+00, -6.7586e-01,  6.4589e-01,  3.2640e-01,\n",
      "        -7.4867e-01,  6.7585e-01,  9.9469e-01, -5.3812e-01, -9.2381e-01,\n",
      "        -4.2445e-01, -2.6701e-01, -5.7261e-01,  3.9967e-01,  5.0608e-01,\n",
      "        -1.6453e-02,  3.1527e-01, -1.2763e+00,  9.1606e-01, -7.6159e-02,\n",
      "        -5.0747e-01, -3.6732e-01,  1.8161e-01, -3.5841e-01, -5.4424e-01,\n",
      "         2.6100e-01, -5.4432e-01,  9.9129e-01,  5.6860e-02,  7.5466e-01,\n",
      "        -6.7616e-01,  5.3304e-01, -1.0713e+00,  1.8274e+00,  1.0923e+00,\n",
      "         1.5836e+00, -9.4519e-01,  1.6245e-01, -6.4333e-02, -1.8237e+00,\n",
      "         6.1454e-01,  7.7781e-02,  6.0585e-01,  2.1424e-01,  8.6889e-02,\n",
      "         1.1978e+00,  1.1860e+00,  5.7142e-04,  5.7663e-02, -4.4496e-01,\n",
      "        -8.9290e-01,  1.6065e-01, -1.1018e+00, -5.2224e-01, -3.4467e-01,\n",
      "         8.0591e-01,  1.3219e+00, -1.5670e+00, -1.3879e-01,  7.1336e-01,\n",
      "         9.2058e-01, -6.3079e-01,  9.5702e-01,  8.6372e-01,  3.9135e-01,\n",
      "         4.4177e-02,  7.8393e-01,  2.8317e-01,  1.0292e-01,  2.7463e-01,\n",
      "        -5.9726e-01,  1.9365e-01,  6.8338e-01,  1.5483e+00,  8.5893e-01,\n",
      "        -1.1020e-01, -2.5002e-01,  1.4640e+00,  4.5182e-01,  4.3631e-01,\n",
      "        -6.4059e-01,  3.2962e-01, -9.4540e-02, -1.7471e-01,  3.8297e-01,\n",
      "         2.5654e-01,  1.6768e-01, -1.6864e+00, -4.9098e-01, -4.5659e-01,\n",
      "         8.3271e-01, -4.3293e-02, -1.0185e+00, -7.1663e-01, -1.1801e-01,\n",
      "        -2.7032e-01, -3.0987e-01, -1.1842e+00,  8.0141e-01, -1.5839e-01,\n",
      "         2.2065e-01, -2.0609e-01,  9.4821e-02, -7.1114e-01, -1.4432e+00,\n",
      "        -3.1837e-01, -9.6587e-01,  7.0050e-02, -2.1263e-01,  1.3987e+00,\n",
      "         1.1488e-01,  1.1892e+00,  9.7470e-02, -6.1829e-01,  1.7487e-01,\n",
      "        -4.7435e-01, -2.9313e-01,  3.3267e-01,  8.0603e-01, -1.7225e-01,\n",
      "         8.6986e-01,  1.1426e+00, -1.3210e-01,  8.3988e-01, -2.6642e-01,\n",
      "        -1.0783e-01,  5.7479e-01,  9.3891e-02,  8.2382e-01,  4.0128e-01,\n",
      "        -5.7094e-01, -9.1363e-01,  3.5843e-01, -6.5940e-01,  2.4037e-01,\n",
      "        -3.1089e-01,  3.5720e-01,  1.7707e-01, -2.3589e-01, -2.7114e-01,\n",
      "        -3.1228e-01,  7.8544e-01,  6.0201e-01,  1.3202e+00, -1.2686e-01,\n",
      "         5.2070e-01,  2.4881e-01,  3.6836e-01, -3.7454e-01, -3.5288e-01,\n",
      "        -7.3989e-01,  1.1822e-01,  2.9934e-01, -1.1649e+00, -3.6853e-01,\n",
      "         2.6530e-01,  1.2052e+00, -4.0287e-01,  1.6986e-01, -6.9786e-01,\n",
      "         7.5252e-01,  7.5357e-01, -1.7214e-01,  1.0023e+00,  1.0615e+00,\n",
      "         2.1016e+00,  4.8308e-01, -5.4692e-01, -5.6937e-01, -7.7214e-01,\n",
      "         4.0350e-01,  7.5108e-01,  2.9226e-01, -5.5857e-01,  2.4853e-01,\n",
      "        -1.3725e+00, -8.2571e-02,  5.7778e-01,  5.3609e-02,  3.7167e-01,\n",
      "        -5.1043e-01,  2.8967e-01, -1.2923e+00,  9.9121e-02,  1.4164e+00,\n",
      "        -7.5805e-02,  7.4710e-02, -4.0330e-01,  1.2158e+00,  2.3088e-01,\n",
      "        -3.8966e-01, -1.3045e+00,  4.1786e-01,  1.5446e-01, -9.0244e-02,\n",
      "        -1.9127e-01,  4.1387e-01, -6.0660e-01,  5.1450e-02,  8.9290e-02,\n",
      "         5.3389e-02,  9.4966e-01,  7.8568e-02,  4.3743e-01, -6.0686e-01,\n",
      "        -2.3886e-01,  6.4684e-01, -6.3899e-02, -3.4756e-01, -6.4378e-01,\n",
      "         4.1361e-01, -7.9401e-01,  1.2310e-02,  7.7207e-01,  1.5338e-01,\n",
      "        -1.3649e+00,  1.4960e-02,  1.8982e-01,  6.1106e-01, -3.5658e-01,\n",
      "         1.1594e+00,  1.0091e+00,  2.4347e-01, -3.6078e-01, -7.7290e-01,\n",
      "         1.1427e+00,  2.7504e-01, -4.6780e-01, -4.0521e-01,  3.7876e-01,\n",
      "         6.3022e-01, -5.8443e-01, -3.5310e-01, -7.7958e-01,  5.0582e-02,\n",
      "         1.2703e+00, -1.6333e-01,  1.0540e-01,  1.1913e+00, -4.6697e-01,\n",
      "        -1.6692e-01, -1.6685e-01, -9.3205e-01, -3.1204e-01, -2.2183e-01,\n",
      "        -4.5302e-01, -5.2378e-01,  6.8934e-01, -1.7199e-01, -7.1560e-01,\n",
      "         5.2393e-01, -1.4757e-01,  3.7427e-01, -4.6827e-01,  2.1599e-01,\n",
      "         1.1299e+00, -9.4785e-02, -9.2611e-01,  9.9669e-01, -1.5120e-01,\n",
      "         4.6048e-02,  5.2086e-02,  2.7016e-01, -6.7088e-02, -6.7841e-01,\n",
      "         1.3725e+00,  2.0032e-01, -4.4457e-01, -1.7599e-01, -9.8935e-01,\n",
      "        -5.5734e-01, -1.9739e+00, -1.8531e-01, -3.1998e-02, -4.2983e-01,\n",
      "        -1.0673e+00,  2.6349e-01, -5.0549e-01, -1.0952e+00,  2.4303e-01,\n",
      "        -1.0481e+00, -4.0656e-01,  4.5503e-01, -4.9650e-01,  9.5187e-01,\n",
      "        -5.2057e-01, -7.2342e-01,  5.5690e-01, -2.0092e-01, -5.2054e-02,\n",
      "         1.0193e+00,  3.0027e-01, -1.7270e-01,  1.7610e-01,  8.1017e-01,\n",
      "        -2.2411e-01, -7.3265e-01, -5.6134e-01,  3.2845e-01, -7.1809e-02,\n",
      "         1.1247e-01,  1.2005e+00,  5.9695e-01,  1.1566e-01,  8.7799e-01,\n",
      "         1.6536e+00,  1.4942e+00,  1.8253e-01,  1.5008e+00,  1.3941e-01,\n",
      "        -3.6162e-01, -1.8017e-01, -2.0540e-01, -9.2303e-02, -7.1547e-01,\n",
      "        -1.4872e+00, -9.1710e-01,  6.6647e-01,  5.2470e-01,  1.9546e-01,\n",
      "        -8.7820e-01, -1.8486e+00,  5.0563e-02, -1.2681e+00, -1.3611e-01,\n",
      "         1.3225e+00, -1.3855e+00, -4.1593e-01,  1.7690e+00,  5.5654e-04,\n",
      "        -3.3305e-02,  7.2132e-01, -1.1276e+00,  3.1322e-01,  1.1521e-01,\n",
      "         9.2172e-01,  3.4287e-01, -1.1208e+00,  3.5748e-01,  8.5205e-01,\n",
      "         1.1740e+00, -6.8028e-02,  1.6041e+00,  7.0351e-01, -1.0012e+00,\n",
      "         3.6861e-01,  1.1805e+00, -1.7342e-01, -4.9306e-01, -9.1565e-01,\n",
      "         8.2872e-01, -1.6764e+00,  2.5763e+00, -1.1292e-01,  9.5330e-01,\n",
      "        -7.1676e-01,  3.4787e-01, -1.9725e-01, -3.2637e-02, -1.1213e+00,\n",
      "        -4.4774e-01,  7.8422e-01, -2.5125e-01, -1.7941e-01, -9.8437e-01,\n",
      "        -5.3692e-01,  8.5041e-01, -3.0220e-01, -2.5987e-01, -4.7431e-02,\n",
      "         5.3547e-01, -7.4570e-01, -3.8569e-01, -2.3359e-01, -2.2744e-02,\n",
      "        -1.3677e-01,  3.4745e-02, -8.7310e-01,  6.8840e-01,  6.9269e-01,\n",
      "        -1.6466e-01, -7.5228e-01,  7.8120e-01,  1.6384e+00,  1.1001e+00,\n",
      "        -2.2803e-01, -9.5778e-01, -5.3499e-01, -7.1744e-01, -2.8244e-01,\n",
      "        -9.0346e-01, -1.5475e-01, -3.3622e-01,  3.8651e-01,  2.6148e-01,\n",
      "        -5.0333e-01,  1.0683e-01, -1.4675e+00,  4.0566e-01,  5.4404e-01,\n",
      "        -2.8159e-01, -4.5310e-02,  3.9861e-01,  2.0429e-02,  4.4088e-01,\n",
      "         6.7670e-01,  1.4783e+00, -4.7121e-01, -8.2155e-02, -1.1909e+00,\n",
      "        -6.9492e-02,  3.5824e-01, -4.5338e-01, -3.5841e-01, -9.4186e-02,\n",
      "        -4.0721e-01,  8.0063e-01,  5.2316e-01,  3.7805e-02,  1.0970e-01,\n",
      "         2.0876e-03, -3.4683e-01,  4.8836e-01,  9.7019e-01, -1.3175e+00,\n",
      "         5.8365e-01, -7.1769e-01,  5.9552e-01, -6.3385e-01, -4.7216e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "target\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GraphVAE(hparams=hparams).to(device=device)\n",
    "# TODO: add beta_1 = 0.5 as in the paper\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n",
    "epochs = hparams[\"epochs\"]\n",
    "train_loader = dataloaders[\"train_tiny\"]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_index, train_batch in enumerate(tqdm(train_loader,  desc=f\"Epoch {epoch + 1} Training\")):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_batch)\n",
    "        a, f, e = out\n",
    "\n",
    "        a = a[0]\n",
    "\n",
    "        adj_triu_mat, batch = train_batch.adj_triu_mat, train_batch.batch\n",
    "\n",
    "        adj_triu_mat = adj_triu_mat[0]\n",
    "\n",
    "        print(\"pred\")\n",
    "        print(a)\n",
    "\n",
    "        print(\"target\")\n",
    "        print(adj_triu_mat)\n",
    "\n",
    "        \n",
    "        break\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
